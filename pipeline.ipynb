{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6481bb",
   "metadata": {},
   "source": [
    "# Low power person detection on UAVs\n",
    "\n",
    "This is the notebook with holds the complete pipeline for our project in TinyML!\n",
    "\n",
    "Our goal is to deploy optimized person detection models on edge devices with regards to trade-offs between power-consumption, inference speed and accuracy. \n",
    "\n",
    "We load, retrain, optimize, benchmark and deploy these models in this jupyternotebook, which is a compressed version of the source code in this repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0595225",
   "metadata": {},
   "source": [
    "## We start with loading different models \n",
    "At the start of the project we used models from EfficientDet, Fomo, Yolo and mobilenet_ssd. After comparison we decided to only move forward with the YOLO model, therefore later code is written only for the yolo architecture.\n",
    "\n",
    "For EfficientDet and Mobilenet-ssd we use tensorflow-hub to get the models, while FOMO is only available via manual download from Edge-Impulse. The usage of YOLO is greatly simplified by using the ultralytics library for YOLO, which handles download and provides a training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12255af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model.py\n",
    "\n",
    "def load_yolo(model_name : str, model_name_ext: str):\n",
    "    \"\"\"\n",
    "    Loads a YOLO modle\n",
    "    \"\"\"\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    model = YOLO(model_name_ext)\n",
    "    exported_path = model.export(format=\"saved_model\")\n",
    "\n",
    "    return exported_path\n",
    "\n",
    "\n",
    "def load_mobilenet_ssd(model_name: str, model_url: str = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\"):\n",
    "    \"\"\"Loads MobileNet SSD from TensorFlow Hub\"\"\"\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    model = hub.load(model_url)\n",
    "    saved_model_path = f\"{model_name}_saved_model\"\n",
    "    tf.saved_model.save(model, saved_model_path)\n",
    "    return saved_model_path\n",
    "\n",
    "\n",
    "def load_efficientdet(model_name: str, model_url: str = \"https://tfhub.dev/tensorflow/efficientdet/d0/1\"):\n",
    "    \"\"\"Loads EfficientDet from TensorFlow Hub\"\"\"\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    model = hub.load(model_url)\n",
    "    saved_model_path = f\"{model_name}_saved_model\"\n",
    "    tf.saved_model.save(model, saved_model_path)\n",
    "    return saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f99185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# download the yolo model\n",
    "model_name = \"yolo11n\"\n",
    "model_name_ext = \"yolo11n.pt\"\n",
    "yolo_saved_model_path = load_yolo(model_name, model_name_ext)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aeca6a",
   "metadata": {},
   "source": [
    "We now downloaded Yolo11n.pt and will continue by using one of two domain-specific datasets to retrain and with that finetune these models for the deployment of person-detection on UAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(\"PyTorch CUDA: \", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "model_name = \"yolo11n\"\n",
    "dataset_name = \"visdrone\"\n",
    "data_path = \"train/visdrone.yaml\"\n",
    "image_size = 640\n",
    "epochs = 100\n",
    "\n",
    "def train(model_name: str, dataset_name:str, data_path: str, image_size: int, epochs: int):\n",
    "    # Load a pretrained model\n",
    "    model = YOLO(model_name + \".pt\")\n",
    "    # Train the model using a custom dataset\n",
    "    results = model.train(\n",
    "        data=data_path, \n",
    "        device=0,\n",
    "        epochs=epochs,\n",
    "        imgsz=image_size,\n",
    "        batch=16,\n",
    "        plots=True,\n",
    "        project=\"../models\",\n",
    "        name= model_name + \"_\" + dataset_name + \"_\" + str(image_size) + \"p_\" + str(epochs) + \"ep\"\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def export(data_path: str, best_model_path: str, image_size: int):\n",
    "    best_model = YOLO(best_model_path)\n",
    "    best_model.export(format=\"onnx\")\n",
    "    best_model.export(\n",
    "        format=\"tflite\",\n",
    "        imgsz=image_size,\n",
    "        # project=\"../models\",\n",
    "        # name=\"yolo11n_fp32_visdrone\"\n",
    "    )\n",
    "    best_model.export(\n",
    "        format=\"tflite\",\n",
    "        imgsz=image_size,\n",
    "        half=True,\n",
    "        # project=\"../models\",\n",
    "        # name=\"yolo11n_fp16_visdrone\"\n",
    "    )\n",
    "    best_model.export(\n",
    "        format=\"tflite\",\n",
    "        imgsz=image_size,\n",
    "        int8=True,\n",
    "        data=data_path,\n",
    "        # project=\"../models\",\n",
    "        # name=\"yolo11n_int8_visdrone\"\n",
    "    )\n",
    "\n",
    "from multiprocessing import freeze_support\n",
    "freeze_support()\n",
    "\n",
    "print(\"Starting training \", model_name, \" on dataset \", dataset_name,\n",
    "      \" for \", epochs, \" epochs with imgsz \", image_size, \"p.\")\n",
    "\n",
    "results = train(model_name, dataset_name, data_path, image_size, epochs)\n",
    "print(results)\n",
    "\n",
    "best_model_path = f\"../models/{model_name}_{dataset_name}_{image_size}p_{epochs}ep/weights/best.pt\"\n",
    "\n",
    "export(data_path, best_model_path, image_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a02fd",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "\n",
    "After retraining on the domain-specific dataset, we optimize the model for edge deployment and produce six TFLite variants per model:\n",
    "\n",
    "\n",
    "- float32: default, experimental\n",
    "\n",
    "\n",
    "- float16 weights-only: default, experimental\n",
    "\n",
    "\n",
    "- dynamic range int8 (int8 weights, float32 I/O): default, experimental\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Key settings:\n",
    "\n",
    "\n",
    "- We use `tf.lite.Optimize.DEFAULT` for all conversions. The older `OPTIMIZE_FOR_SIZE` and `OPTIMIZE_FOR_LATENCY` are deprecated and behave the same as `DEFAULT`.\n",
    "\n",
    "\n",
    "- Experimental variants add `tf.lite.Optimize.EXPERIMENTAL_SPARSITY`, which leverages pruned (sparse) weights if the SavedModel was trained with pruning. Note: conversion does not perform pruning itself.\n",
    "\n",
    "\n",
    "- For float16, we convert weights to fp16 but keep inference input/output types as float32 to match the SavedModel signature.\n",
    "\n",
    "\n",
    "- We restrict to TFLite built-in ops (`TFLITE_BUILTINS`) and attempt the experimental converter for broader support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_model(\n",
    "    model_name: str,\n",
    "    model_path: str,\n",
    "    output_dir: str = \"models/optimized_models\",\n",
    "    \n",
    "):\n",
    "    \"\"\"Generate optimized TFLite variants and optionally verify sizes.\n",
    "   \"\"\"\n",
    "  \n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # f32 ( baseline )\n",
    "    fp32_default = _convert(\n",
    "        model_name, model_path, output_dir, \"float32_default\", tf.lite.Optimize.DEFAULT, None, use_sparsity=False\n",
    "    )\n",
    "    fp32_experimental = _convert(\n",
    "        model_name, model_path, output_dir, \"float32_experimental\", tf.lite.Optimize.DEFAULT, None, use_sparsity=True\n",
    "    )\n",
    "\n",
    "    # Float16 (but IO f32)\n",
    "    fp16_default = _convert(\n",
    "        model_name, model_path, output_dir, \"float16_default\", tf.lite.Optimize.DEFAULT, tf.float16, use_sparsity=False\n",
    "    )\n",
    "    fp16_experimental = _convert(\n",
    "        model_name, model_path, output_dir, \"float16_experimental\", tf.lite.Optimize.DEFAULT, tf.float16, use_sparsity=True\n",
    "    )\n",
    "\n",
    "    # Dynamic range int8 (weights int8, float32 IO)\n",
    "    dyn_int8_default = _convert(\n",
    "        model_name, model_path, output_dir, \"dynamic_int8_default\", tf.lite.Optimize.DEFAULT, \"dynamic\", use_sparsity=False\n",
    "    )\n",
    "    dyn_int8_experimental = _convert(\n",
    "        model_name, model_path, output_dir, \"dynamic_int8_experimental\", tf.lite.Optimize.DEFAULT, \"dynamic\", use_sparsity=True\n",
    "    )\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def _convert(model_name: str, model_path: str, output_dir: str, suffix: str, optimization, quant_type, use_sparsity: bool = False):\n",
    "    \"\"\"Helper to convert a model with specific settings.\n",
    "\n",
    "    If use_sparsity=True we include EXPERIMENTAL_SPARSITY with DEFAULT so that sparse\n",
    "    weights ( if they are present ) are encoded more efficiently. This does not perform pruning.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "\n",
    "\n",
    "    optimizations = [optimization] if optimization else [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "    if use_sparsity and tf.lite.Optimize.EXPERIMENTAL_SPARSITY not in optimizations:\n",
    "        # Combine DEFAULT + EXPERIMENTAL_SPARSITY when requested.\n",
    "        optimizations.append(tf.lite.Optimize.EXPERIMENTAL_SPARSITY)\n",
    "\n",
    "\n",
    "    converter.optimizations = optimizations\n",
    "\n",
    "    # restrict to tensorflow builtin ops\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "\n",
    "    # float16 quantization\n",
    "    if quant_type == tf.float16:\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    elif quant_type == \"dynamic\":\n",
    "        # Dynamic range quantization always uses DEFAULT; include sparsity if requested.\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT] + ([tf.lite.Optimize.EXPERIMENTAL_SPARSITY] if use_sparsity else [])\n",
    "    \n",
    "    # try with experimental converter\n",
    "    converter.experimental_new_converter = True\n",
    "\n",
    "\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "    except ValueError as e:\n",
    "\n",
    "        print(f\"Skipping {suffix} for {model_name}: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{model_name}_{suffix}.tflite\")\n",
    "\n",
    "    with open(output_path, 'wb') as f:\n",
    "\n",
    "        f.write(tflite_model)\n",
    "\n",
    "\n",
    "    print(f\"{suffix}: {output_path} ({len(tflite_model) / (1024*1024):.2f} MB)\")\n",
    "\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b560783",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_model(model_name, best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ab7f7",
   "metadata": {},
   "source": [
    "Choosing a optimized model :( SOME MODEL )\n",
    "we can now run some inference on example pictures to see where stengths and weaknesses of our detection lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e4ca4",
   "metadata": {},
   "source": [
    "To gain actual knowledge we run different benchmarks on our models to compare them performance and power wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e13bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:23<00:00,  2.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inference_time (ms)': 132.99864292144775,\n",
       " 'memory_usage (MiB)': 910.1628125,\n",
       " 'cpu_temperature (C)': 64.192,\n",
       " 'cpu_usage (%)': 82.64,\n",
       " 'energy_consumption (W)': 3.5660000000000003}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Model\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "import numpy as np\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "P_idle = 2.7\n",
    "P_max = 7.0\n",
    "\n",
    "def get_model_type(model_path: str) -> str:\n",
    "    if 'yolo' in model_path:\n",
    "        return 'yolo'\n",
    "    elif 'fomo' in model_path:\n",
    "        return 'fomo'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_CPU_temp():\n",
    "    res = os.popen('vcgencmd measure_temp').readline()\n",
    "    return float(res.replace(\"temp=\",\"\").replace(\"'C\\n\",\"\"))\n",
    "\n",
    "def benchmark(model_path: str, images_path: str, amount_of_images):\n",
    "    model_type = get_model_type(model_path)\n",
    "    model = Model(model_type=model_type, path=model_path)\n",
    "\n",
    "    time_values = []\n",
    "    memory_values = []\n",
    "    temp_values = []\n",
    "    cpu_usage_values = []\n",
    "    energy_values = []\n",
    "    \n",
    "    for image in tqdm(os.listdir(images_path)[:amount_of_images]):\n",
    "        image_path = os.path.join(images_path, image)\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.inference(img, postprocess=False)\n",
    "        end_time = time.time()\n",
    "\n",
    "\n",
    "        mem_usage = memory_usage((model.inference, (img, False, )))\n",
    "\n",
    "        time_values.append((end_time - start_time) * 1000)\n",
    "        cpu_usage_values.append(psutil.cpu_percent())\n",
    "        temp_values.append(get_CPU_temp())\n",
    "        memory_values.append(np.mean(mem_usage))\n",
    "        energy_values.append(P_idle + (P_max - P_idle) * (cpu_usage_values[-1] / 100))\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'inference_time (ms)': np.mean(time_values),\n",
    "        'memory_usage (MiB)': np.mean(memory_values),\n",
    "        'cpu_temperature (C)': np.mean(temp_values),\n",
    "        'cpu_usage (%)': np.mean(cpu_usage_values),\n",
    "        'energy_consumption (W)': np.mean(energy_values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c221ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
